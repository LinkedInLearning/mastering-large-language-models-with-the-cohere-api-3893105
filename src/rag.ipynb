{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz33G3t6gbOl"
      },
      "source": [
        "# RAG\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rACbepFGgbOo"
      },
      "outputs": [],
      "source": [
        "! pip install cohere -q # we'll get some wikipedia data\n",
        "! pip install wikipedia -qq\n",
        "! pip install -qU langchain-text-splitters -qq\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import wikipedia\n",
        "import cohere\n",
        "co = cohere.ClientV2(\"\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP-bWt9XgbOq",
        "outputId": "fe883159-062e-4f47-fb89-87af814ee2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text has roughly 2291 words.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "article = wikipedia.page('Geoffrey_Hinton')\n",
        "text = article.content\n",
        "print(f\"The text has roughly {len(text.split())} words.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhXW7iHC1-Q6",
        "outputId": "2687c6e9-6dea-45a8-b27a-88e11dcec92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text has been broken down in 115 chunks.\n"
          ]
        }
      ],
      "source": [
        "# Create basic configurations to chunk the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=196,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Split the text into chunks with some overlap\n",
        "chunks_ = text_splitter.create_documents([text])\n",
        "chunks = [c.page_content for c in chunks_]\n",
        "print(f\"The text has been broken down in {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8g0sE2hgbOs"
      },
      "source": [
        "### Embed every text chunk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEarMPEqgbOs",
        "outputId": "f04f50eb-25b8-47f8-bdbe-ec589d280806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL: 115 embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
        "model = \"embed-english-v3.0\"\n",
        "\n",
        "def batch_embed(texts, batch_size=96):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        response = co.embed(\n",
        "            texts=batch,\n",
        "            model=model,\n",
        "            input_type=\"search_document\",\n",
        "            embedding_types=['float']\n",
        "        )\n",
        "        all_embeddings.extend(response.embeddings.float)\n",
        "    return all_embeddings\n",
        "\n",
        "embeddings = batch_embed(chunks)\n",
        "print(f\"TOTAL: {len(embeddings)} embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM6vKeypgbOs"
      },
      "source": [
        "\n",
        "\n",
        "We use a python dictionary using `np.array()` to store the embeddings however you can use Pinecone, Zillis or any other offering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "sdW7M8HLvB-9"
      },
      "outputs": [],
      "source": [
        "\n",
        "! pip install numpy -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "H2srFH-IgbOs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "vector_database = {i: np.array(embedding) for i, embedding in enumerate(embeddings)}\n",
        "# { 0: array([...]), 1: array([...]), 2: array([...]), ..., 10: array([...]) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Y2HTxspKgbOs"
      },
      "outputs": [],
      "source": [
        "query = \"What does the E. in Hintons name signify?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrUuS6vXgbOs",
        "outputId": "a2b59bef-b798-4e3c-e9ae-a5b7d5e2de5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_embedding:  [-0.040527344, 0.056488037, -0.018753052, 0.04434204, 0.01914978, -0.012611389, 0.022644043, -0.07165527, 0.019515991, 0.03640747, '...']\n"
          ]
        }
      ],
      "source": [
        "# Because the text being embedded is the search query, we set the input type as search_query\n",
        "response = co.embed(\n",
        "    texts=[query],\n",
        "    model=model,\n",
        "    input_type=\"search_query\",\n",
        "    embedding_types=['float']\n",
        ")\n",
        "query_embedding = response.embeddings.float[0]\n",
        "print(\"query_embedding: \", query_embedding[:10] + [\"...\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K8B87CGgbOt"
      },
      "source": [
        "### Retrieve the most relevant chunks from the vector database\n",
        "\n",
        "We use cosine similarity to find the most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nik3es32gbOt",
        "outputId": "76710708-d11e-4fc7-9546-89d7f68285f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity scores:  [0.4500334860991959, 0.13559572292793903, 0.4860002246235057, 0.1565135127099831, 0.14155310531493967, 0.4034877337600028, 0.2997154100141264, 0.13241436399228365, 0.11363173423445763, 0.40725143036338013, 0.11689773610942873, 0.12668441729257257, 0.43125451565105066, 0.13262501831355825, 0.08763032134075259, 0.05402140099064808, 0.2248387361064034, 0.4973361904507819, 0.15435095968226026, 0.14973796821981455, 0.07770539497942422, 0.14255262560867377, 0.42460580513546087, 0.1519891479701841, 0.0912597925737139, 0.1093380691315491, 0.3664181316009147, 0.3197753831824755, 0.34015310494593787, 0.3402271859804888, 0.06421820357873181, 0.4199617384515595, 0.1000726099767655, 0.3991104194983768, 0.35524473922358685, 0.30598889390294143, 0.13691185057723185, 0.060737921016011706, 0.0988810467290436, 0.42483969864347454, 0.2863442906978318, 0.30432680684362956, 0.11566861353297109, 0.36414068929569354, 0.29126973095529085, 0.1303286469472496, 0.07871657879218447, 0.0631873066226767, 0.45246394928443867, 0.07413547526378873, 0.1308667653865139, 0.11782161460507985, 0.16180138380882855, 0.43390590406513235, 0.42027205641611715, 0.06600059347463172, 0.1362842250092046, 0.10385125845389419, 0.10752543432688912, 0.4723594068393545, 0.37026054309423034, 0.4656808156575739, 0.20371398026772183, 0.1560780438825329, 0.1500014811698645, 0.10685279092444168, 0.41977450548682316, 0.16538454898433613, 0.15609775469278192, 0.13104264119973322, 0.03924979155411201, 0.16239851829726393, 0.34961482933714555, 0.3233915203120044, 0.10361718086641478, 0.053541063351524254, 0.41531794362225044, 0.10153980714704126, 0.09518722997343346, 0.44411601299717446, 0.09171325501264359, 0.3825667984812354, 0.229571910924103, 0.12130249870487418, 0.12929708799880266, 0.08542825330217095, 0.3526019566505864, 0.3463158039826012, 0.08515172808789885, 0.25376378265714766, 0.20998687842979516, 0.08079083418306895, 0.27150056716080234, 0.3297163321773385, 0.09368827339590162, 0.3863830235948456, 0.11201852072426279, 0.3000520329367086, 0.0577248026458992, 0.39427968098057997, 0.336034781017344, 0.32046337052923524, 0.08192294936944287, 0.3963714466022585, 0.4174333772126567, 0.10883506169101283, 0.08291836441513986, 0.4359348787295669, 0.49607432530186846, 0.3611165179525231, 0.35068388505497644, 0.5248352123116722, 0.21317821087739888, 0.16994654692325564, 0.08596077938065413]\n",
            "Here are the indices of the top 10 chunks after retrieval:  [111  17 108   2  59  61  48   0  79 107]\n",
            "Here are the top 10 chunks after retrieval: \n",
            "== Hinton's father was the entomologist Howard Hinton. His middle name comes from another relative, George Everest, the Surveyor General of India after whom the mountain is named. He is the nephew\n",
            "== Hinton was educated at Clifton College in Bristol and the University of Cambridge as an undergraduate student of King's College, Cambridge. After repeatedly changing his degree between different\n",
            "== Hinton is the great-great-grandson of the mathematician and educator Mary Everest Boole and her husband, the logician George Boole. George Boole's work eventually became one of the foundations of\n",
            "== Hinton is University Professor Emeritus at the University of Toronto. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly\n",
            "== In 2001, Hinton was awarded an honorary doctorate from the University of Edinburgh. He was the 2005 recipient of the IJCAI Award for Research Excellence lifetime-achievement award. He was awarded\n",
            "== Killam Prize in Engineering. In 2013, Hinton was awarded an honorary doctorate from the Universit√© de Sherbrooke.\n",
            "== In May 2023, Hinton publicly announced his resignation from Google. He explained his decision by saying that he wanted to \"freely speak out about the risks of A.I.\" and added that a part of him\n",
            "== Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian computer scientist, cognitive scientist, cognitive psychologist, known for his work on artificial neural networks which earned\n",
            "== In an interview with The New York Times published on 1 May 2023, Hinton announced his resignation from Google so he could \"talk about the dangers of AI without considering how this impacts\n",
            "== == Personal life ==\n",
            "Hinton's second wife, Rosalind Zalin, died of ovarian cancer in 1994; his third wife, Jackie, died in September 2018, also of cancer.\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Calculate similarity between the user question & each chunk\n",
        "similarities = [cosine_similarity(query_embedding, chunk) for chunk in embeddings]\n",
        "print(\"similarity scores: \", similarities)\n",
        "\n",
        "# Get indices of the top 10 most similar chunks\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Keep only the top 10 indices\n",
        "top_indices = sorted_indices[:10]\n",
        "print(\"Here are the indices of the top 10 chunks after retrieval: \", top_indices)\n",
        "\n",
        "# Retrieve the top 10 most similar chunks\n",
        "top_chunks_after_retrieval = [chunks[i] for i in top_indices]\n",
        "print(\"Here are the top 10 chunks after retrieval: \")\n",
        "for t in top_chunks_after_retrieval:\n",
        "    print(\"== \" + t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcpds3VgbOt"
      },
      "source": [
        "## Rerank the chunks retrieved from the vector database\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J4LywVygbOt",
        "outputId": "f3f91ac4-ebd2-49e7-b39e-c02b4ef10709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the top 3 chunks after rerank: \n",
            "== Hinton's father was the entomologist Howard Hinton. His middle name comes from another relative, George Everest, the Surveyor General of India after whom the mountain is named. He is the nephew\n",
            "== Hinton is the great-great-grandson of the mathematician and educator Mary Everest Boole and her husband, the logician George Boole. George Boole's work eventually became one of the foundations of\n",
            "== Hinton is University Professor Emeritus at the University of Toronto. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly\n"
          ]
        }
      ],
      "source": [
        "response = co.rerank(\n",
        "    query=query,\n",
        "    documents=top_chunks_after_retrieval,\n",
        "    top_n=3,\n",
        "    model=\"rerank-english-v3.0\",\n",
        ")\n",
        "\n",
        "top_chunks_after_rerank = [top_chunks_after_retrieval[result.index] for result in response.results]\n",
        "\n",
        "print(\"Here are the top 3 chunks after rerank: \")\n",
        "for t in top_chunks_after_rerank:\n",
        "    print(\"== \" + t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "oCNXWH8GgbOt"
      },
      "outputs": [],
      "source": [
        "# preamble containing instructions about the task and the desired style for the output.\n",
        "preamble = \"\"\"\n",
        "## Task & Context\n",
        "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
        "\n",
        "## Style Guide\n",
        "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BevatShtgbOt",
        "outputId": "cefb25b4-376c-42f1-b858-35ff63d50cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final answer:\n",
            "Assuming you are referring to Hinton's surname, the 'E' stands for Everest, after George Everest, the Surveyor General of India.\n",
            "\n",
            "Hinton is the great-great-grandson of the mathematician and educator Mary Everest Boole and her husband, the logician George Boole.\n"
          ]
        }
      ],
      "source": [
        "# retrieved documents\n",
        "documents = [\n",
        "    {\"data\": {\"title\": \"chunk 0\", \"snippet\": top_chunks_after_rerank[0]}},\n",
        "    {\"data\": {\"title\": \"chunk 1\", \"snippet\": top_chunks_after_rerank[1]}},\n",
        "    {\"data\": {\"title\": \"chunk 2\", \"snippet\": top_chunks_after_rerank[2]}},\n",
        "  ]\n",
        "\n",
        "# get model response\n",
        "response = co.chat(\n",
        "  model=\"command-r-08-2024\",\n",
        "  messages=[{\"role\" : \"system\", \"content\" : preamble},\n",
        "            {\"role\" : \"user\", \"content\" : query}],\n",
        "  documents=documents,\n",
        "  temperature=0.3\n",
        ")\n",
        "\n",
        "print(\"Final answer:\")\n",
        "print(response.message.content[0].text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}