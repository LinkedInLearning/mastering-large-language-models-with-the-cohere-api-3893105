{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz33G3t6gbOl"
      },
      "source": [
        "# RAG\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rACbepFGgbOo"
      },
      "outputs": [],
      "source": [
        "! pip install cohere -q # we'll get some wikipedia data\n",
        "! pip install wikipedia -qq\n",
        "! pip install -qU langchain-text-splitters -qq\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import wikipedia\n",
        "import cohere\n",
        "co = cohere.ClientV2(\"\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP-bWt9XgbOq",
        "outputId": "fe883159-062e-4f47-fb89-87af814ee2c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "article = wikipedia.page('Geoffrey_Hinton')\n",
        "text = article.content\n",
        "print(f\"The text has roughly {len(text.split())} words.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhXW7iHC1-Q6",
        "outputId": "2687c6e9-6dea-45a8-b27a-88e11dcec92b"
      },
      "outputs": [],
      "source": [
        "# Create basic configurations to chunk the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Split the text into chunks with some overlap\n",
        "chunks_ = text_splitter.create_documents([text])\n",
        "chunks = [c.page_content for c in chunks_]\n",
        "print(f\"The text has been broken down in {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8g0sE2hgbOs"
      },
      "source": [
        "### Embed every text chunk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEarMPEqgbOs",
        "outputId": "f04f50eb-25b8-47f8-bdbe-ec589d280806"
      },
      "outputs": [],
      "source": [
        "# Because the texts being embedded are the chunks we are searching over, we set the input type as search_doc\n",
        "model = \"embed-english-v3.0\"\n",
        "\n",
        "def batch_embed(texts, batch_size=96):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        response = co.embed(\n",
        "            texts=batch,\n",
        "            model=model,\n",
        "            input_type=\"search_document\",\n",
        "            embedding_types=['float']\n",
        "        )\n",
        "        all_embeddings.extend(response.embeddings.float)\n",
        "    return all_embeddings\n",
        "\n",
        "embeddings = batch_embed(chunks)\n",
        "print(f\"TOTAL: {len(embeddings)} embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM6vKeypgbOs"
      },
      "source": [
        "\n",
        "\n",
        "We use a python dictionary using `np.array()` to store the embeddings however you can use Pinecone, Zillis or any other offering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "sdW7M8HLvB-9"
      },
      "outputs": [],
      "source": [
        "\n",
        "! pip install numpy -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "H2srFH-IgbOs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "vector_database = {i: np.array(embedding) for i, embedding in enumerate(embeddings)}\n",
        "# { 0: array([...]), 1: array([...]), 2: array([...]), ..., 10: array([...]) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Y2HTxspKgbOs"
      },
      "outputs": [],
      "source": [
        "query = \"What does the E. in Hintons name signify?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrUuS6vXgbOs",
        "outputId": "a2b59bef-b798-4e3c-e9ae-a5b7d5e2de5a"
      },
      "outputs": [],
      "source": [
        "# Because the text being embedded is the search query, we set the input type as search_query\n",
        "response = co.embed(\n",
        "    texts=[query],\n",
        "    model=model,\n",
        "    input_type=\"search_query\",\n",
        "    embedding_types=['float']\n",
        ")\n",
        "query_embedding = response.embeddings.float[0]\n",
        "print(\"query_embedding: \", query_embedding[:10] + [\"...\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K8B87CGgbOt"
      },
      "source": [
        "### Retrieve the most relevant chunks from the vector database\n",
        "\n",
        "We use cosine similarity to find the most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nik3es32gbOt",
        "outputId": "76710708-d11e-4fc7-9546-89d7f68285f5"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Calculate similarity between the user question & each chunk\n",
        "similarities = [cosine_similarity(query_embedding, chunk) for chunk in embeddings]\n",
        "print(\"similarity scores: \", similarities)\n",
        "\n",
        "# Get indices of the top 10 most similar chunks\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Keep only the top 10 indices\n",
        "top_indices = sorted_indices[:10]\n",
        "print(\"Here are the indices of the top 10 chunks after retrieval: \", top_indices)\n",
        "\n",
        "# Retrieve the top 10 most similar chunks\n",
        "top_chunks_after_retrieval = [chunks[i] for i in top_indices]\n",
        "print(\"Here are the top 10 chunks after retrieval: \")\n",
        "for t in top_chunks_after_retrieval:\n",
        "    print(\"== \" + t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcpds3VgbOt"
      },
      "source": [
        "## Rerank the chunks retrieved from the vector database\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J4LywVygbOt",
        "outputId": "f3f91ac4-ebd2-49e7-b39e-c02b4ef10709"
      },
      "outputs": [],
      "source": [
        "response = co.rerank(\n",
        "    query=query,\n",
        "    documents=top_chunks_after_retrieval,\n",
        "    top_n=3,\n",
        "    model=\"rerank-english-v3.0\",\n",
        ")\n",
        "\n",
        "top_chunks_after_rerank = [top_chunks_after_retrieval[result.index] for result in response.results]\n",
        "\n",
        "print(\"Here are the top 3 chunks after rerank: \")\n",
        "for t in top_chunks_after_rerank:\n",
        "    print(\"== \" + t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "oCNXWH8GgbOt"
      },
      "outputs": [],
      "source": [
        "# preamble containing instructions about the task and the desired style for the output.\n",
        "preamble = \"\"\"\n",
        "## Task & Context\n",
        "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
        "\n",
        "## Style Guide\n",
        "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BevatShtgbOt",
        "outputId": "cefb25b4-376c-42f1-b858-35ff63d50cec"
      },
      "outputs": [],
      "source": [
        "# retrieved documents\n",
        "documents = [\n",
        "    {\"data\": {\"title\": \"chunk 0\", \"snippet\": top_chunks_after_rerank[0]}},\n",
        "    {\"data\": {\"title\": \"chunk 1\", \"snippet\": top_chunks_after_rerank[1]}},\n",
        "    {\"data\": {\"title\": \"chunk 2\", \"snippet\": top_chunks_after_rerank[2]}},\n",
        "  ]\n",
        "\n",
        "# get model response\n",
        "response = co.chat(\n",
        "  model=\"command-r-08-2024\",\n",
        "  messages=[{\"role\" : \"system\", \"content\" : preamble},\n",
        "            {\"role\" : \"user\", \"content\" : query}],\n",
        "  documents=documents,\n",
        "  temperature=0.3\n",
        ")\n",
        "\n",
        "print(\"Final answer:\")\n",
        "print(response.message.content[0].text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
